{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2do3n4JqpI"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This project uses an LLM (via the OpenAI API) to extract commonly asked questions from unstructured text data and automatically creates a Notion page (via the Notion API) to document them.\n",
        "\n",
        "The Slack message data used in this example was originally extracted using **Fivetran**.  However, the code is **designed to work with any text-based message data**. You can adapt it easily to any dataset containing text conversations (e.g., customer chats, support tickets, forum posts).\n",
        "\n",
        "The results created by the automated Notion integration can be viewed here:  \n",
        "üîó [View Results on Notion](https://www.notion.so/Auto-generated-FAQ-1afd8cbfdbfd80279fa5c9200855c19b)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HcLWj0WfHp_"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "To run this notebook, you will need:\n",
        "\n",
        "- A database or file containing the text message data you want to analyze.\n",
        "- In this example, Slack message data (`team_slack`) is loaded from **Google Drive**.  \n",
        "  ‚ûî You will need to replace this part with your own data loading method, depending on where your messages are stored (e.g., local file, database, or cloud storage).\n",
        "- An **OpenAI API key** for processing and summarizing your text data.\n",
        "- A **Notion API integration**, which provides you with a Notion \"secret\" (your Notion API key), for automatically exporting the results to Notion.\n",
        "- An empty **Notion page** to export your results to.\n",
        "\n",
        "üëâ **Important:**  \n",
        "- You must have a **billed OpenAI account** (free-tier accounts without billing enabled cannot use the API).\n",
        "- Running this notebook will **incur API costs** based on the number of tokens processed.  \n",
        "  Costs depend on the size of your message dataset and the OpenAI model you use.\n",
        "- You can create and manage your OpenAI API key here: [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)\n",
        "\n",
        "- You can create a **Notion integration** here: [https://www.notion.so/my-integrations](https://www.notion.so/my-integrations).  \n",
        "  After creating the integration, copy your **Internal Integration Token** (the Notion secret/API key), and  \n",
        "  ‚ûî **share the target Notion page** with your integration to give it the necessary permissions.\n",
        "\n",
        "Please make sure you have both API keys ready before running the code.  \n",
        "You can set them securely as environment variables (recommended) or insert them directly into the code (not recommended for shared environments).\n",
        "\n",
        "Example environment variables:\n",
        "- `OPENAI_API_KEY`\n",
        "- `NOTION_API_KEY`\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDPasz20MMRI"
      },
      "source": [
        "# **Cost Estimate**\n",
        "\n",
        "Running this code on approximately 2,000 text messages using the OpenAI `gpt-4o` model cost me less than **$1** in API usage.\n",
        "\n",
        "üëâ **Important:** Actual costs will vary depending on:\n",
        "- The total number of tokens processed ‚Äî which depends heavily on **how many messages** you process and **how long each message is** (more text = more tokens).\n",
        "- The model used (e.g., GPT-4o is cheaper than GPT-4-turbo).\n",
        "\n",
        "üõ°Ô∏è **Tip:**  \n",
        "To control spending, you can disable \"Auto-recharge\" in your OpenAI billing settings and/or set a usage limit.  \n",
        "Always monitor your usage in the [OpenAI usage dashboard](https://platform.openai.com/usage) to avoid unexpected charges.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U30hmVKvhQNl"
      },
      "source": [
        "# A Note on Batch Size and Temperature\n",
        "\n",
        "Batch size and temperature are parameters set inside the LLM function to control how the messages are processed and how consistent or creative the model's responses are.\n",
        "\n",
        "\n",
        "### 1Ô∏è‚É£ Batch Size: Affects Context and Accuracy\n",
        "\n",
        "**What It Does:**\n",
        "\n",
        "* Determines how many messages are processed in a single LLM call.\n",
        "* Larger batches provide more context, allowing the model to find recurring themes more effectively.\n",
        "* Smaller batches may miss broader patterns because the LLM sees less data at once.\n",
        "\n",
        "**Impact of Batch Size:**\n",
        "\n",
        "| Batch Size | Pros | Cons |\n",
        "|------------|---------------------------------|------------------------------------------------|\n",
        "| **Small (e.g., 10-20 messages per batch)** | Faster responses, easier error handling | Less context, less effective at spotting trends |\n",
        "| **Medium (e.g., 50-100 messages per batch, default)** | Balance of context and performance | Still may not catch all patterns |\n",
        "| **Large (e.g., 200+ messages per batch)** | More context, better at summarization | Slower response, might hit token limits |\n",
        "\n",
        "**Best Practice Summary for Batch Size:**\n",
        "\n",
        "* ‚úÖ Use medium or large batch sizes (50-200 messages per batch)\n",
        "* ‚úÖ If patterns aren't emerging, try increasing the batch size so the model sees more context.\n",
        "* ‚úÖ If API calls are slow or expensive, reduce batch size to process in chunks.\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Approach Chosen In This Case: Combine batch 100 & batch 300 results**\n",
        ">\n",
        "> Combining the results of batch size 100 and batch size 300 into a single, balanced summary, using LLM. This allows the LLM to merge the high-level themes from batch 300 with the more detailed breakdown from batch 100, resulting in a structured but not overly detailed summary.\n",
        "\n",
        "### 2Ô∏è‚É£ Temperature: Affects Consistency vs. Creativity\n",
        "\n",
        "**What It Does:**\n",
        "\n",
        "* Controls how random or deterministic the responses are.\n",
        "* Lower values (0.1 - 0.3) ‚Üí More consistent, structured, and predictable outputs.\n",
        "* Higher values (0.7 - 1.0) ‚Üí More creative, diverse, and sometimes inconsistent results.\n",
        "\n",
        "**Impact of Temperature:**\n",
        "\n",
        "| Temperature | Behavior | Use Case |\n",
        "|-------------|-----------------------------------|------------------------------------------------------------|\n",
        "| **0.1 - 0.3** | Consistent, structured, focused | Best for summaries, factual outputs, and categorization |\n",
        "| **0.4 - 0.6** | Some variation, slight randomness | Good for brainstorming alternative groupings |\n",
        "| **0.7 - 1.0** | Highly creative, less structured | Best for generating unique ideas, but not for structured summarization |\n",
        "\n",
        "\n",
        "**Best Practice Summary for Temperature settings:**\n",
        "\n",
        "* ‚úÖ Use a low temperature (0.2 - 0.3) to get a clear, structured summary.\n",
        "* ‚úÖ If the model's output is too rigid, try increasing the temperature slightly (0.4 - 0.5).\n",
        "* ‚úÖ If the output is too chaotic or inconsistent, lower the temperature to 0.1 - 0.2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u2Ifdt_e2cn"
      },
      "source": [
        "# Here Comes the Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjTc-lBmJqpL"
      },
      "source": [
        "## Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRtYxTaxJBbx"
      },
      "outputs": [],
      "source": [
        "# Install the OpenAI Python package if you haven't already\n",
        "# (You can run this in your terminal or notebook once)\n",
        "# pip install openai\n",
        "\n",
        "# Import necessary libraries\n",
        "import openai  # For interacting with the OpenAI API\n",
        "import pandas as pd  # For working with tabular data (e.g., storing and processing messages)\n",
        "import os  # For accessing environment variables (e.g., loading the OpenAI API key) and handling file paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAwQ4O2cJX-A"
      },
      "source": [
        "## Set Up LLM Access (OpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNoX10VQJMJO"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"insert_your_openai_api_key\" # Insert your OpenAI api key here\n",
        "\n",
        "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHApePi9Jaqb"
      },
      "source": [
        "## Load the message data\n",
        "\n",
        "‚ö° Note: In this example, the Slack messages (\"team_slack\") are loaded from Google Drive.\n",
        "\n",
        "Replace this with your own data loading method if your messages are stored elsewhere (e.g., local CSV, database query, other cloud storage).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfxf4B79KJz6",
        "outputId": "933a5b62-b852-4984-fa6d-4e74785b9dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EamVRWWeJrz4"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/My Drive/data/team_slack_20250224.csv\"\n",
        "team_slack = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za4XCgMlKTjz",
        "outputId": "fa56a185-7d52-461b-f140-b83ef3abe4ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1864 entries, 0 to 1863\n",
            "Data columns (total 20 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   message_channel_id  1864 non-null   object \n",
            " 1   parent_message_ts   1864 non-null   float64\n",
            " 2   ts                  1864 non-null   float64\n",
            " 3   user_id             1864 non-null   object \n",
            " 4   subtype             197 non-null    object \n",
            " 5   last_read           0 non-null      float64\n",
            " 6   subscribed          291 non-null    object \n",
            " 7   source_team         0 non-null      float64\n",
            " 8   type                1864 non-null   object \n",
            " 9   team                1543 non-null   object \n",
            " 10  is_locked           291 non-null    object \n",
            " 11  edited_user         176 non-null    object \n",
            " 12  _fivetran_deleted   1864 non-null   bool   \n",
            " 13  _fivetran_synced    1864 non-null   object \n",
            " 14  edited_ts           176 non-null    float64\n",
            " 15  reply_count         291 non-null    float64\n",
            " 16  thread_ts           1586 non-null   float64\n",
            " 17  reply_users_count   291 non-null    float64\n",
            " 18  latest_reply        291 non-null    float64\n",
            " 19  masked_text         1817 non-null   object \n",
            "dtypes: bool(1), float64(9), object(10)\n",
            "memory usage: 278.6+ KB\n"
          ]
        }
      ],
      "source": [
        "team_slack.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Mh2XHLJqpM"
      },
      "source": [
        "## Preprocess data\n",
        "\n",
        "This step applies basic cleaning to the Slack messages:\n",
        "- Removes any leading or trailing spaces\n",
        "- Ensures that only valid text strings are processed (non-strings are replaced with an empty string)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8p6HCOzJURV"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the 'masked_text' column and store the cleaned text in a new column 'clean_text'\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic cleaning of Slack messages\"\"\"\n",
        "    if isinstance(text, str):  # Ensure it's a string before processing\n",
        "        return text.strip()  # Remove leading/trailing spaces\n",
        "    return \"\"\n",
        "\n",
        "team_slack['clean_text'] = team_slack['masked_text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZDO2vc_NctI"
      },
      "source": [
        "## Use OpenAI to extract common questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncqr2C8MTthe"
      },
      "source": [
        "### LLM Function and Prompt\n",
        "\n",
        "The function below sends a structured prompt to the OpenAI model (default: GPT-4o) to identify and group actual questions from a batch of Slack messages into an FAQ-style format.\n",
        "\n",
        "In the development process, I ran several versions of the prompt and gradually refined it to improve the results.\n",
        "\n",
        "The prompt was improved step-by-step to ensure:\n",
        "* ‚úÖ Focus on extracting **actual questions** from the text (not just themes or summaries)\n",
        "* ‚úÖ **Grouping similar questions** together into a coherent FAQ format\n",
        "* ‚úÖ **Removing unnecessary thematic descriptions** and ensuring the questions are **clear and concise**\n",
        "\n",
        "In this cleaned-up version, only the final, optimized prompt is included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3Cdh0rW3KO6"
      },
      "outputs": [],
      "source": [
        "def extract_faq_questions(messages, model=\"gpt-4o\"):\n",
        "    \"\"\"Extract actual FAQ-style common questions from Slack messages\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are analyzing Slack messages to extract **common recurring questions** that team members frequently ask.\n",
        "\n",
        "### **Instructions:**\n",
        "- Extract **only actual questions** (e.g., \"How do I get API access?\" instead of \"Discussing API access\").\n",
        "- Group them into **clear FAQ categories**.\n",
        "- Do **not include summaries or descriptions**‚Äîonly list questions.\n",
        "- Do **not switch to general topic descriptions**‚Äîalways extract full-sentence questions.\n",
        "\n",
        "### **Expected Output Format:**\n",
        "\n",
        "**1. Meeting Schedules & Availability**\n",
        "- What time works for everyone for the next team meeting?\n",
        "- Can we schedule a 1-1 session?\n",
        "- Is there a slot available to meet before our scheduled call?\n",
        "\n",
        "**2. API Access & Authentication**\n",
        "- How do I obtain OAuth credentials for API access?\n",
        "- What permissions are required to authenticate via the API?\n",
        "- How do I troubleshoot OAuth authentication failures?\n",
        "\n",
        "**3. Debugging & Technical Issues**\n",
        "- How do I resolve an SSL protocol error?\n",
        "- What's the best way to debug OAuth authentication issues?\n",
        "- How do I fix issues with database connections?\n",
        "\n",
        "**4. Project Management & Task Coordination**\n",
        "- How should we organize project tasks in Notion?\n",
        "- What is the best practice for managing Git branches?\n",
        "- How do we handle merging conflicts effectively?\n",
        "\n",
        "---\n",
        "\n",
        "**Here are the extracted Slack messages:**\n",
        "\n",
        "{messages}\n",
        "\n",
        "**Now, extract and format only direct questions under their respective categories. Do NOT include descriptions or summaries. Avoid switching to topic descriptions.**\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # client = openai.OpenAI(api_key=\"your-api-key-here\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in identifying and structuring common questions in team discussions.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3  # Ensures structured, non-random responses\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content  # Extract the FAQ-style questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOEWH3nXJqpN"
      },
      "source": [
        "### Run with batch size 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyI11Olx3WBS"
      },
      "outputs": [],
      "source": [
        "def batch_messages_100(df, column='masked_text', batch_size=100):\n",
        "    \"\"\"Processes messages in batches and extracts common questions\"\"\"\n",
        "    questions = []\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        batch = \"\\n\".join(df[column][i:i+batch_size].dropna().tolist())  # Drop NaNs before joining\n",
        "        result = extract_faq_questions(batch)\n",
        "        questions.append(result)\n",
        "    return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q70OAjd4d9J"
      },
      "outputs": [],
      "source": [
        "faq_b100 = batch_messages_100(team_slack, column='masked_text')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGowq6Sa4pVL",
        "outputId": "73c623b5-0c24-4718-dad7-75d0e058d1e7"
      },
      "outputs": [],
      "source": [
        "faq_b100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U4Bi8veWnlH"
      },
      "source": [
        "### Run with batch size 300\n",
        "\n",
        "Larger batches provide more context, allowing the model to find recurring themes more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7vstuo0TsIa"
      },
      "outputs": [],
      "source": [
        "def batch_messages_300(df, column='masked_text', batch_size=300):\n",
        "    \"\"\"Processes messages in batches and extracts common questions\"\"\"\n",
        "    questions = []\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        batch = \"\\n\".join(df[column][i:i+batch_size].dropna().tolist())  # Drop NaNs before joining\n",
        "        result = extract_faq_questions(batch)\n",
        "        questions.append(result)\n",
        "    return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPjvmVzl-PHM",
        "outputId": "2577ef6f-cb6e-4073-8379-fdacfb4d73db"
      },
      "outputs": [],
      "source": [
        "faq_b300 = batch_messages_300(team_slack, column='masked_text')\n",
        "faq_b300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qL9s4YrZaY2"
      },
      "source": [
        "### Merge b100 and b300 results\n",
        "\n",
        "Combining the results of batch size 100 and batch size 300 into a single, balanced summary. This allows the LLM to merge the high-level themes from batch 300 with the more detailed breakdown from batch 100, resulting in a structured but not overly detailed summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maauEEHvZevc"
      },
      "outputs": [],
      "source": [
        "def merge_summaries(faq_b100, faq_b300, model=\"gpt-4o\"):\n",
        "    \"\"\"Merge and refine the two batch summaries into a single, balanced list.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are analyzing two summaries of common Slack discussions focussing on common questions raised:\n",
        "\n",
        "    1. **The first summary (Batch 300)** is a high-level overview of the most recurring questions.\n",
        "    2. **The second summary (Batch 100)** is a more detailed breakdown of different questions.\n",
        "\n",
        "    Your task is to merge these into a **single, refined summary** that:\n",
        "    - Keeps the most important topics and questions from Batch 300.\n",
        "    - Retains useful details from Batch 100 without excessive repetition.\n",
        "    - Ensures topics are **not too broad, but also not overly detailed**.\n",
        "    - Groups similar questions together.\n",
        "    - Avoids listing redundant information.\n",
        "\n",
        "    Here are the summaries:\n",
        "\n",
        "    **Batch 300 Summary:**\n",
        "    {faq_b300}\n",
        "\n",
        "    **Batch 100 Summary:**\n",
        "    {faq_b100}\n",
        "\n",
        "    Please return the merged summary in a structured format with grouped categories.\n",
        "    \"\"\"\n",
        "\n",
        "    # client = openai.OpenAI(api_key=\"your-api-key-here\")  # Ensure API key is set\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are an AI expert at refining and merging topic summaries.\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3  # Keep it structured and deterministic\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content  # Extract refined summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKDM-aa8aAnf"
      },
      "outputs": [],
      "source": [
        "merged_faqs = merge_summaries(faq_b100, faq_b300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doVC29MCavCk"
      },
      "source": [
        "# Summary of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn9WkhQpU_nM"
      },
      "source": [
        "Please note:\n",
        "Here I **manually removed the last category (\"General Inquiries\")** because it was a little messy and included a mix of internal feedback and random one-off asks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nL-XowVU-K4"
      },
      "outputs": [],
      "source": [
        "def remove_last_category(merged_faqs_text, category_title=\"**6. General Inquiries**\"):\n",
        "    \"\"\"Removes the last unwanted category and anything after it from the merged FAQ text.\"\"\"\n",
        "    if category_title in merged_faqs_text:\n",
        "        # Only keep the part before the unwanted category\n",
        "        cleaned_text = merged_faqs_text.split(category_title)[0].strip()\n",
        "        return cleaned_text\n",
        "    else:\n",
        "        # If the category is not found, return the original text\n",
        "        return merged_faqs_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcjdfX4NVQa9",
        "outputId": "4bd2ae78-2dc7-431f-924c-af42e11e5181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Merged Summary of Slack Discussions**\n",
            "\n",
            "**1. Meeting Schedules & Availability**\n",
            "- What time works for everyone for the next team meeting?\n",
            "- Can we schedule a 1-1 session?\n",
            "- Is there a slot available to meet before our scheduled call?\n",
            "- Should we have a meeting tomorrow morning?\n",
            "- Please let me know what hours work for you, so that we can plan team calls accordingly.\n",
            "- Can you please record the meeting?\n",
            "- Does anyone have schedule conflicts with the time?\n",
            "\n",
            "**2. API Access & Authentication**\n",
            "- How do I obtain OAuth credentials for API access?\n",
            "- What permissions are required to authenticate via the API?\n",
            "- How do I troubleshoot OAuth authentication failures?\n",
            "- Could you check if there‚Äôs an issue with the access or if I need to take any additional steps?\n",
            "- Do I need to create a new Slack app to obtain the required API access token?\n",
            "\n",
            "**3. Debugging & Technical Issues**\n",
            "- How do I resolve an SSL protocol error?\n",
            "- What's the best way to debug OAuth authentication issues?\n",
            "- How do I fix issues with database connections?\n",
            "- Is there a chance you have another instance of the application running?\n",
            "- Did you configure the Doppler CLI with the new login?\n",
            "- Can you specify the exact page you tried to access?\n",
            "\n",
            "**4. Project Management & Task Coordination**\n",
            "- How should we organize project tasks in Notion?\n",
            "- What is the best practice for managing Git branches?\n",
            "- How do we handle merging conflicts effectively?\n",
            "- Should I merge my code now and make another PR to follow the guide, or make the changes first?\n",
            "- Can you please ensure this is updated in Notion?\n",
            "\n",
            "**5. Tools & Software Usage**\n",
            "- Does anyone have good practical knowledge of Airtable?\n",
            "- Do you use BigQuery, or which programs/platforms do you use for SQL queries?\n",
            "- Make sure that you have access to ChatGPT Pro. Those who haven't, please let me know in the thread.\n"
          ]
        }
      ],
      "source": [
        "# Remove the 6th category before printing or uploading\n",
        "cleaned_merged_faqs = remove_last_category(merged_faqs)\n",
        "\n",
        "# Now print the cleaned version\n",
        "print(cleaned_merged_faqs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umrtl92gu_g4"
      },
      "source": [
        "# Notion Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ursK_Eb-vD33"
      },
      "outputs": [],
      "source": [
        "# pip install notion-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I83dJKrmxWwX"
      },
      "outputs": [],
      "source": [
        "from notion_client import Client\n",
        "import os\n",
        "\n",
        "# --- SETUP: Replace with your credentials ---\n",
        "NOTION_API_KEY = \"insert_your_own_Notion_API_key_here\"  # Replace with your API key\n",
        "NOTION_PAGE_ID = \"1afd8cbfdbfd80279fa5c9200855c19b\"  # Replace with the Notion Page ID\n",
        "\n",
        "# --- Initialize Notion Client ---\n",
        "notion = Client(auth=NOTION_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBqlAuAaQm-n"
      },
      "outputs": [],
      "source": [
        "# The output generated by the LLM is in Markdown-style formatting.\n",
        "# This function parses the Markdown into a list of Notion blocks.\n",
        "\n",
        "def parse_markdown_to_notion_blocks(markdown_text):\n",
        "    \"\"\"\n",
        "    Parses simple Markdown text into a list of Notion blocks (headings and bullet list items).\n",
        "    Only handles bold headings (**) and bullets (-).\n",
        "    \"\"\"\n",
        "\n",
        "    notion_blocks = []\n",
        "    lines = markdown_text.splitlines()\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue  # skip empty lines\n",
        "\n",
        "        # Heading detection: lines starting and ending with **\n",
        "        if line.startswith(\"**\") and line.endswith(\"**\"):\n",
        "            heading_text = line.strip(\"*\").strip()\n",
        "            block = {\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"heading_2\",\n",
        "                \"heading_2\": {\n",
        "                    \"rich_text\": [\n",
        "                        {\"type\": \"text\", \"text\": {\"content\": heading_text}}\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "            notion_blocks.append(block)\n",
        "\n",
        "        # Bullet detection: lines starting with -\n",
        "        elif line.startswith(\"-\"):\n",
        "            bullet_text = line.lstrip(\"-\").strip()\n",
        "            block = {\n",
        "                \"object\": \"block\",\n",
        "                \"type\": \"bulleted_list_item\",\n",
        "                \"bulleted_list_item\": {\n",
        "                    \"rich_text\": [\n",
        "                        {\"type\": \"text\", \"text\": {\"content\": bullet_text}}\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "            notion_blocks.append(block)\n",
        "\n",
        "        # Otherwise, ignore the line (safe for now)\n",
        "\n",
        "    return notion_blocks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBbGYy8FQ92t"
      },
      "outputs": [],
      "source": [
        "# Function to Add Parsed Content to Notion Page\n",
        "def add_blocks_to_notion(blocks, page_id):\n",
        "    \"\"\"Uploads a list of Notion blocks to the specified Notion page.\"\"\"\n",
        "\n",
        "    # Append blocks to the Notion page\n",
        "    notion.blocks.children.append(page_id, children=blocks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn8_BzSqQ-Ut",
        "outputId": "fb3991b0-cd5c-4d2a-bd20-58ac1ce8ee6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data successfully added to Notion!\n"
          ]
        }
      ],
      "source": [
        "# Parse the merged_faqs markdown into structured Notion blocks\n",
        "notion_blocks = parse_markdown_to_notion_blocks(cleaned_merged_faqs)\n",
        "\n",
        "# Upload the structured blocks to Notion\n",
        "add_blocks_to_notion(notion_blocks, NOTION_PAGE_ID)\n",
        "\n",
        "print(\"‚úÖ Data successfully added to Notion!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
